{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import subprocess\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "from ecg_datasets import ECGDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import wfdb\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from hpt.models.policy import Policy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "\n",
    "def print_ts(obj):\n",
    "    print(type(obj))\n",
    "    print(obj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lead_names = [\"I\", \"II\", \"III\", \"aVR\", \"aVL\", \"aVF\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\"]\n",
    "sampling_rate=100\n",
    "start_time = 0\n",
    "time = 10\n",
    "start_length = int(start_time * sampling_rate)\n",
    "sample_length = int(time * sampling_rate)\n",
    "end_time = start_time + time\n",
    "t = np.arange(start_time, end_time, 1 / sampling_rate)\n",
    "data_path = '../../ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def draw_ecg(ecg, lead=1):\n",
    "    print(type(ecg))\n",
    "    print(ecg.shape)\n",
    "    plt.plot(\n",
    "        t,\n",
    "        ecg[lead][start_length: start_length + sample_length],\n",
    "        linewidth=2,\n",
    "        color=\"k\",\n",
    "        alpha=1.0,\n",
    "        label=lead_names[lead]\n",
    "    )\n",
    "    minimum = min(ecg[lead])\n",
    "    maximum = max(ecg[lead])\n",
    "    ylims_candidates = [-2.5, -2.0, -1.5, -1.0, -0.5, 0, 0.5, 1.0 , 1.5, 2.0, 2.5]\n",
    "\n",
    "    ylims = (\n",
    "        max([x for x in ylims_candidates if x <= minimum]),\n",
    "        min([x for x in ylims_candidates if x >= maximum]),\n",
    "    )\n",
    "    plt.vlines(np.arange(start_time, end_time, 0.2), ylims[0], ylims[1], colors=\"r\", alpha=1.0)\n",
    "    plt.vlines(np.arange(start_time, end_time, 0.04), ylims[0], ylims[1], colors=\"r\", alpha=0.3)\n",
    "    plt.hlines(np.arange(ylims[0], ylims[1], 0.5), start_time, end_time, colors=\"r\", alpha=1.0)\n",
    "    plt.hlines(np.arange(ylims[0], ylims[1], 0.1), start_time, end_time, colors=\"r\", alpha=0.3)\n",
    "\n",
    "    plt.xticks(np.arange(start_time, end_time + 1, 1.0))\n",
    "    plt.margins(0.0)\n",
    "    plt.show()\n",
    "\n",
    "def draw_ecgs(ecgs, lead=1):\n",
    "    for i, ecg in enumerate(ecgs):\n",
    "        plt.rcParams[\"figure.figsize\"] = (25, 1.5 * len(ecgs))\n",
    "        plt.subplot(len(ecgs), 1, i + 1)\n",
    "        draw_ecg(ecg, lead)\n",
    "\n",
    "def visualize_sample(sample_data, lead):\n",
    "    ecgs = []\n",
    "    for i, ecg_path in enumerate(sample_data[\"ecg_path\"]):\n",
    "        if not os.path.exists(ecg_path + \".dat\"):\n",
    "            download_ptbxl(sample_data[\"ecg_id\"][i])\n",
    "\n",
    "        ecg, _ = wfdb.rdsamp(ecg_path)\n",
    "        ecgs.append(ecg.T)\n",
    "\n",
    "    draw_ecgs(ecgs, lead)\n",
    "    print(f\"Sample ID: {sample_data['sample_id']}\")\n",
    "    print(f\"Question: {sample_data['question']}\")\n",
    "    print(f\"Answer: {sample_data['answer']}\")\n",
    "\n",
    "def get_ptbxl_data_path(ecg_id):\n",
    "    return os.path.join(\n",
    "        f\"{int(ecg_id / 1000) * 1000 :05d}\",\n",
    "        f\"{ecg_id:05d}_hr\"\n",
    "    )\n",
    "\n",
    "def download_ptbxl(ecg_id):\n",
    "    ptbxl_data_path = get_ptbxl_data_path(ecg_id)\n",
    "    !wget -r -N -c np https://physionet.org/files/ptb-xl/1.0.3/records500/{ptbxl_data_path}.hea -P ptbxl &> /dev/null\n",
    "    shutil.move(\n",
    "        os.path.join(\"ptbxl\", f\"physionet.org/files/ptb-xl/1.0.3/records500/{get_ptbxl_data_path(ecg_id)}.hea\"),\n",
    "        os.path.join(\"ptbxl\")\n",
    "    )\n",
    "\n",
    "    !wget -r -N -c np https://physionet.org/files/ptb-xl/1.0.3/records500/{ptbxl_data_path}.dat -P ptbxl &> /dev/null\n",
    "    shutil.move(\n",
    "        os.path.join(\"ptbxl\", f\"physionet.org/files/ptb-xl/1.0.3/records500/{get_ptbxl_data_path(ecg_id)}.dat\"),\n",
    "        os.path.join(\"ptbxl\")\n",
    "    )\n",
    "    shutil.rmtree(os.path.join(\"ptbxl\", \"physionet.org\"))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def aggregate_diagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in agg_df.index:\n",
    "            tmp.append(agg_df.loc[key].diagnostic_class)\n",
    "    return list(set(tmp))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = \"/home/qfbqt/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# load and convert annotation data\n",
    "Y = pd.read_csv(data_path+'ptbxl_database.csv', index_col='ecg_id')\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Load raw signal data\n",
    "X = load_raw_data(Y, sampling_rate, data_path)\n",
    "\n",
    "# Load scp_statements.csv for diagnostic aggregation\n",
    "agg_df = pd.read_csv(data_path+'scp_statements.csv', index_col=0)\n",
    "agg_df = agg_df[agg_df.diagnostic == 1]\n",
    "Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_diagnostic)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"labels_array = Y['diagnostic_superclass'].values\n",
    "#remove the entries without any superclass\n",
    "mask = [len(labels) > 0 for labels in labels_array]\n",
    "mask_array = np.array(mask)\n",
    "ecgdata_filtered = X[mask_array]\n",
    "labels_filtered = labels_array[mask_array]\n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = MultiLabelBinarizer()\n",
    "labels_filtered_encoded = label_encoder.fit_transform(labels_filtered)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"with open('ecgdata_filtered.pk', 'wb') as f:\n",
    "    pickle.dump(ecgdata_filtered, f)\n",
    "\n",
    "# Save labels_filtered_encoded to 'labels_filtered_encoded.pk'\n",
    "with open('labels_filtered_encoded.pk', 'wb') as f:\n",
    "    pickle.dump(labels_filtered_encoded, f)\n",
    "\n",
    "print(\"Objects have been saved successfully.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecgdata_filtered = None\n",
    "labels_filtered_encoded = None\n",
    "\n",
    "# Check and load ecgdata_filtered\n",
    "if os.path.exists('ecgdata_filtered.pk'):\n",
    "    with open('ecgdata_filtered.pk', 'rb') as f:\n",
    "        ecgdata_filtered = pickle.load(f)\n",
    "    print(\"Loaded ecgdata_filtered from file.\")\n",
    "else:\n",
    "    print(\"File 'ecgdata_filtered.pk' does not exist.\")\n",
    "\n",
    "# Check and load labels_filtered_encoded\n",
    "if os.path.exists('labels_filtered_encoded.pk'):\n",
    "    with open('labels_filtered_encoded.pk', 'rb') as f:\n",
    "        labels_filtered_encoded = pickle.load(f)\n",
    "    print(\"Loaded labels_filtered_encoded from file.\")\n",
    "else:\n",
    "    print(\"File 'labels_filtered_encoded.pk' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_dataset = ECGDataset(ecgdata_filtered, labels_filtered_encoded)\n",
    "train_dataset, test_dataset, valid_dataset = torch.utils.data.random_split(ecg_dataset, [0.7, 0.2, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the normalizer to use for validation and test sets\n",
    "normalizer = ecg_dataset.get_normalizer()\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\"\"\"\n",
    "batch_size = 16  # Adjust as needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('experiments/configs/config.yaml')\n",
    "policy = Policy(embed_dim=128, num_blocks=4, num_heads=4, action_horizon=0)\n",
    "# Assuming 'ecg_dataset' is your domain name\n",
    "policy.init_domain_stem(domain_name='ecg_dataset', stem_spec=cfg.stem_ecg)\n",
    "# Initialize domain head with the normalizer\n",
    "policy.init_domain_head(\n",
    "    domain_name='ecg_dataset',\n",
    "    normalizer=normalizer,\n",
    "    head_spec=cfg.head_ecg\n",
    ")\n",
    "policy.finalize_modules()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "policy.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(policy.parameters(), lr=cfg.optimizer.lr, weight_decay=cfg.optimizer.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = cfg.train.total_epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    policy.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        #inputs = batch['data'].to(device).float()  # Shape: (batch_size, sequence_length, input_dim)\n",
    "        #labels = batch['label'].to(device).long()  # Shape: (batch_size,)\n",
    "        \n",
    "        #print_ts(inputs)\n",
    "        #print_ts(labels)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = policy.compute_loss(batch)  # Adjust based on your model's expected input\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Optional: Validate the model\n",
    "    # policy.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     # Perform validation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
